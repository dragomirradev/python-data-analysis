{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Inverted Index for search using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted index are table like data-structure associating a given word to a list of document(s) containing it. They are very relevant to modern search engines and variants of it with contextual optimizations are used in many production search engines like Google and Solr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since inverted indices are, usually, built to search natural language documents, we need ways to programmatically analyse natural language texts. Such analysis requires use of statistics, machine learning techniques and datasets to train these models. While, no doubt, Natural Language Processing or (NLP) - field that studies such models and techniques - is itself pretty interesting, it is beyond the scope of our current context. The book - [Information Retrieval](https://nlp.stanford.edu/IR-book/) - is a very accessible starting point for some these techniques and also covers Inverted Indices in the later half of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our immediate requirements we will rely on [NLTK](http://www.nltk.org). It is an open source python package that enables us to work with natural language and provides handy operations like tokenization and lemmatization with pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Install NLTK using pip\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we install `NLTK` using `pip`, we only get the code and not the datasets and pretrained models. Lets download some of them which are relevant to our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below command will open an inline dialog to explore and select packages that are needed. We will select and download `stopwords` and `punkt_tokenizer` packages.\n",
    "\n",
    "There is an alternative command - `nltk.download()` - which will open a graphical user interface to select packages for download. While, some users may find the interface to be convenient, it may be buggy and slow on some machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As has been our approach in our earlier sections, we will construct a fictitious text database to build our inverted index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following texts have been extraced via [Project Gutenberg](https://www.gutenberg.org) from the books:\n",
    "- The Jungle Book (Rudyard Kipling)\n",
    "- Alice in Wonderland (Lewis Carrol)\n",
    "- Concrete Construction Methods and Costs (HP Gillette)\n",
    "- The Telephone (Dolbear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"\"\"\n",
    "    So she swallowed one of the cakes and was delighted to find that she\n",
    "    began shrinking directly. As soon as she was small enough to get through the door,\n",
    "    she ran out of the house and found quite a crowd of little animals and birds waiting outside.\n",
    "    They all made a rush at Alice the moment she appeared, but she ran off as hard as \n",
    "    she could and soon found herself safe in a thick wood.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    If two strips of different metals, such as silver and iron, be soldered together\n",
    "    at one end, and the other ends be connected with a galvanometer, on heating the\n",
    "    soldered junction of the metals it will be found that a current of electricity\n",
    "    traverses the circuit from the iron to the silver. If other metals be used,\n",
    "    having the same size, and the same degree of heat be applied, the current of\n",
    "    electricity thus generated will give a greater or a less deflection, which will be\n",
    "    constant for the metals employed. The two metals generally employed are bismuth\n",
    "    and antimony, in bars about an inch long and an eighth of an inch square.\n",
    "    These are soldered together in series so as to present for faces the ends of the\n",
    "    bars, and these often number as many as fifty pairs. Such a series is called a thermo-pile.\n",
    "    This method of[25] generating electricity was discovered by Seebeck of Berlin in 1821,\n",
    "    but the thermo-pile so much in use now in heat investigations was invented by Nobili in 1835.    \n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    According to this authority a field testing laboratory will cost for equipment $250 to $350.\n",
    "    Such a laboratory can be operated by two or three men at a salary charge of from $100 to $200\n",
    "    per month. Two men will test on an average four samples per day and each additional man will\n",
    "    test four more samples. The cost of testing will range from $3 to $5 per sample, which\n",
    "    is roughly equivalent to 3 cts. per barrel of[Pg 5] cement, or from 3 to 5 cts.per cubic yard\n",
    "    of concrete. These figures are for field laboratory work reasonably well conducted\n",
    "    under ordinarily favorable conditions. In large laboratories the cost per sample will\n",
    "    run somewhat lower.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Just then Alice ran across the Duchess (who was now out of prison). She tucked her arm\n",
    "    affectionately into Alice's and they walked off together. Alice was very glad to find her in\n",
    "    such a pleasant temper. She was a little startled, however, when she heard the voice of the\n",
    "    Duchess close to her ear. \"You're thinking about something, my dear, and that makes you forget to talk.\"\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    It was the jackal—Tabaqui, the Dish-licker—and the wolves of India despise Tabaqui because he \n",
    "    runs about making mischief, and telling tales, and eating rags and pieces of leather from the \n",
    "    village rubbish-heaps. But they are afraid of him too, because Tabaqui, more than \n",
    "    anyone else in the jungle, is apt to go mad, and then he forgets that he was ever afraid of\n",
    "    anyone, and runs through the forest biting everything in his way. Even the tiger runs and\n",
    "    hides when little Tabaqui goes mad, for madness is the most disgraceful thing that can\n",
    "    overtake a wild creature. We call it hydrophobia, but they call it dewanee—the madness—and run.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    A great roofless palace crowned the hill, and the marble of the courtyards and the fountains\n",
    "    was split, and stained with red and green, and the very cobblestones in the courtyard where the\n",
    "    king’s elephants used to live had been thrust up and apart by grasses and young trees.\n",
    "    From the palace you could see the rows and rows of roofless houses that made up the city looking\n",
    "    like empty honeycombs filled with blackness; the shapeless block of stone that had been an idol\n",
    "    in the square where four roads met; the pits and dimples at street corners where the public wells\n",
    "    once stood, and the shattered domes of temples with wild figs sprouting on their sides.\n",
    "    The monkeys called the place their city, and pretended to despise the Jungle-People because they\n",
    "    lived in the forest. And yet they never knew what the buildings were made for nor how to use them.\n",
    "    They would sit in circles on the hall of the king’s council chamber, and scratch for fleas and\n",
    "    pretend to be men; or they would run in and out of the roofless houses and collect pieces of plaster\n",
    "    and old bricks in a corner, and forget where they had hidden them, and fight and cry in scuffling crowds,\n",
    "    and then break off to play up and down the terraces of the king’s garden, where they would shake the\n",
    "    rose trees and the oranges in sport to see the fruit and flowers fall. They explored all the passages\n",
    "    and dark tunnels in the palace and the hundreds of little dark rooms, but they never remembered what\n",
    "    they had seen and what they had not; and so drifted about in ones and twos or crowds telling each\n",
    "    other that they were doing as men did. They drank at the tanks and made the water all muddy,\n",
    "    and then they fought over it, and then they would all rush together in mobs and shout:\n",
    "    \"There is no one in the jungle so wise and good and clever and strong and gentle as the Bandar-log.\"\n",
    "    Then all would begin again till they grew tired of the city and went back to the tree-tops,\n",
    "    hoping the Jungle-People would notice them.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often pronouns, vowels, prepositions like - `a`, `you`, `them` etc. - have little relevance in our search queries. (Unless, of course, we are attempting to isolate the semantic implication of the query.) We can filter out these words to reduce \"noise\" in our index. Such words are called `stopwords`. NLTK provides a dataset of such words for English language which we can use to filter our texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to eliminate `stopwords`, we must first derive words from our corpus. A naive approach could be:\n",
    "\n",
    "```python\n",
    "[ c.split() for c in corpus ]```\n",
    "\n",
    "While this may work with ordinary concatenation of words, it will create problems when our corpus contains punctuations among other artifacts. We use a `tokenizer` - functions based on statistical models to separate paragraphs into sentences or sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will contain a list of all words in the corpus\n",
    "corpus_words = []\n",
    "\n",
    "# Tokenize a paragraph into sentences and each sentence in to\n",
    "# words\n",
    "for c in corpus:\n",
    "    for sent in sent_tokenize(c):\n",
    "        word_tokens = word_tokenize(sent)\n",
    "        corpus_words += word_tokens\n",
    "\n",
    "len(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 427)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets inspect the words in our corpus a bit\n",
    "corpus_words.index('she'), corpus_words.index('She')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words.index('so'), corpus_words.index('So')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data not only contains duplicates but also case sensitive duplicates. Other than in some specialised scenarios, we do not differentiate between upper, lower or mixed case words. So lets convert them all to lower case and eliminate duplicates by making a `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_corpus_words = set([ x.lower() for x in corpus_words ])\n",
    "len(lower_corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stwords = set(stopwords.words('english'))\n",
    "\n",
    "# Using set difference to eliminate stopwords from our words\n",
    "stopfree_words = lower_corpus_words - stwords\n",
    "len(stopfree_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Often same words are spelt and pronounced differently based on grammar tense and the context they are used in. For example: summarisation, summarise are similiar words but reflecting different grammatical context. Whereas the former is a noun, the latter is a verb. In most search scenarios, this difference should not matter.\n",
    "\n",
    "We pass our set of words through a process called `stemming` which reduces words to their common root-word. In our example, `summaris` is the root of summarise and summarisation. This will not only help us in reducing the size of index but also improve search matching by looking \"deeper\" into a word than mere alphabet or string matching.\n",
    "\n",
    "There is no formulae for stemming instead there are approaches to derive them. In our examples, we use the **Snowball Stemmer** built in `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "\n",
    "stemmer = snowball.SnowballStemmer('english')\n",
    "stemmed_words = set([stemmer.stem(x) for x in stopfree_words])\n",
    "len(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced our corpus to about `330` words that we need to look up to match documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voic',\n",
       " 'split',\n",
       " 'investig',\n",
       " 'till',\n",
       " 'passag',\n",
       " 'yard',\n",
       " 'electr',\n",
       " 'cri',\n",
       " 'jackal—tabaqui',\n",
       " 'thrust']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at some of our words\n",
    "list(stemmed_words)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discard whatever we have done until now. Why? Because we never maintained a reference to the document where the word was found. We did so to get a better understanding of each step in our approach to build an inverted index. Lets us now do everything over again, but this time maintaining reference to the document where the word was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our index is a map of word -> documents it is found in\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "# We maintain the reference to the document by its index in the corpus list\n",
    "for docid, c in enumerate(corpus):\n",
    "    for sent in sent_tokenize(c):\n",
    "        for word in word_tokenize(sent):\n",
    "            word_lower = word.lower()\n",
    "            if word_lower not in stwords:\n",
    "                word_stem = stemmer.stem(word_lower)\n",
    "                # We add the document to the set againt the word in our\n",
    "                # index\n",
    "                inverted_index[word_stem].add(docid)\n",
    "\n",
    "len(inverted_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching using the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we processed our documents we need to process our query before looking up in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_search(query):\n",
    "    matched_documents = set()\n",
    "    for word in word_tokenize(query):\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stwords:\n",
    "            word_stem = stemmer.stem(word_lower)\n",
    "            matches = inverted_index.get(word_stem)\n",
    "            if matches:\n",
    "                # The operator |= is a short hand for set union\n",
    "                matched_documents |= matches\n",
    "    return matched_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 3}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_and_search(\"alice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `alice` - as per our index - appears in documents `0` and `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_and_search(\"coward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`coward` does not appear in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4, 5}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_and_search(\"run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run` appears in documents - `2`, `4` and `5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 5}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_and_search(\"metal crowds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used `|=` (`set.union`) operation when processing the query, if a document matches any one of the words in the query, it will be returned in the result. In production grade search engines, we would have to handle requirements where documents need to be returned if they contains both the words or even exact same phrase and many more such conditions.\n",
    "\n",
    "Also our search does not even bother with ranking results - sorting the list of matched documents with document most relevant to the query being the first in the list. Ranking is very important when most of the queries match a very large number of documents.\n",
    "\n",
    "A common approach used for ranking documents is **tf-idf** or **Term Frequency - Inverse Document Frequency**. In web search engines, statistical models like clustering, click analysis and hyperlink references are also used in ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
